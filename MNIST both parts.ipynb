{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import re\n",
    "start_time=time.time()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=center> Train Dataset Preparation </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#edo vazoume to proto etsi gia na kseziparoume mono ta train files\n",
    "import zipfile\n",
    "archive = zipfile.ZipFile('C:/Users/ilias/Desktop/Project ML/mnistdata.zip')\n",
    "for file in archive.namelist():\n",
    "    if file.startswith('train'):\n",
    "        archive.extract(file,\"C:/Users/ilias/Documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'00:01:08'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#edo ginetai to ntaraveri to kalo.\n",
    "start_time_1=time.time()\n",
    "rows=[]\n",
    "train_lengths=[]\n",
    "for i in range(0,10):\n",
    "    #mpainoun ta panta grammi grammi apo ola ta files\n",
    "    with open('train'+str(i)+'.txt') as f:\n",
    "        rows.append(f.readlines())\n",
    "    for j in range(0,len(rows[i])):\n",
    "        #dioxnoume ta \\n\n",
    "        rows[i][j]=rows[i][j][:-1]\n",
    "        #kratame mono ta integers\n",
    "        rows[i][j]=[*map(int, rows[i][j].split())]\n",
    "        #ta kanoume kai np.arrays gia na mas volepsei me to plot\n",
    "        rows[i][j]=np.array(rows[i][j], dtype='uint8')\n",
    "    #edo tou apothikeuo poso mikos exei kathe txt diladi poses eikones periexei gia na exoume kai ta sosta labels\n",
    "    train_lengths.append(len(rows[i]))\n",
    "#ksediplono tin nested lista gia na exoume se kathe grammi mia eikona        \n",
    "train_data=[item for sublist in rows for item in sublist]\n",
    "\n",
    "elapsed_time_1=time.time()-start_time_1 \n",
    "#ola auta ston mageutiko xrono tou enos leptou kai kati sec\n",
    "time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5923, 6742, 5958, 6131, 5842, 5421, 5918, 6265, 5851, 5949]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#edo vlepoume poses eikones apo tin kathe klasi fortosame\n",
    "train_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#edo tsekaroume ean mas efyge kamia eikona (einai ontos 60K)\n",
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def visualization(aux0):\n",
    "    pixels = aux0.reshape((28, 28))\n",
    "    plt.imshow(pixels, cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "train_labels=[]\n",
    "for i in range(0,len(train_lengths)):\n",
    "    train_labels+=list(itertools.repeat(0+i,train_lengths[i]))\n",
    "#h parakato grammi einai optional ean thelo na kano ta labels strings gia na min mpleksoume me ta noumera pou antiprosopeuoun\n",
    "#train_labels=list(map(str, train_labels))\n",
    "\n",
    "#######################\n",
    "labeled_train_data=pd.Series(train_data,index=[train_labels])\n",
    "#####################\n",
    "#auti poia mas dinei oles tis grammes me index 5\n",
    "labeled_train_data[5]\n",
    "#eno auti mas dinei tin 5h eikona\n",
    "labeled_train_data.iloc[5];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=center> Test Dataset Preparation </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "archive = zipfile.ZipFile('C:/Users/ilias/Desktop/Project ML/mnistdata.zip')\n",
    "for file in archive.namelist():\n",
    "    if file.startswith('test'):\n",
    "        archive.extract(file,\"C:/Users/ilias/Documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'00:00:10'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time_2=time.time()\n",
    "rows=[]\n",
    "test_lengths=[]\n",
    "for i in range(0,10):\n",
    "    with open('test'+str(i)+'.txt') as f:\n",
    "        rows.append(f.readlines())\n",
    "    for j in range(0,len(rows[i])):\n",
    "        rows[i][j]=rows[i][j][:-1]\n",
    "        rows[i][j]=[*map(int, rows[i][j].split())]\n",
    "        rows[i][j]=np.array(rows[i][j], dtype='uint8')\n",
    "    test_lengths.append(len(rows[i]))\n",
    "test_data=[item for sublist in rows for item in sublist]\n",
    "elapsed_time_2=time.time()-start_time_2 \n",
    "#ola auta ston mageutiko xrono ton <15 sec\n",
    "time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[980, 1135, 1032, 1010, 982, 892, 958, 1028, 974, 1009]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "test_labels=[]\n",
    "for i in range(0,len(test_lengths)):\n",
    "    test_labels+=list(itertools.repeat(0+i,test_lengths[i]))\n",
    "labeled_test_data=pd.Series(test_data,index=[test_labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align='center'> SGD </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import gzip\n",
    "import numpy\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function for computing the softmax \n",
    "def softmax(x):\n",
    "    \n",
    "    e = numpy.exp(x - numpy.max(x))  # ensure numerical stability\n",
    "#     print('entered softmat')\n",
    "#     print('input of softamax is :\\n',x)\n",
    "#     print('e of softmax is\\n',e)\n",
    "#     print('Sum of es of softmax is,\\n')\n",
    "    if e.ndim == 1:\n",
    "#         print(numpy.sum(e, axis=0))\n",
    "        return e / numpy.sum(e, axis=0)\n",
    "    else:  \n",
    "#         print(numpy.sum(e, axis=1))\n",
    "        return e / numpy.array([numpy.sum(e, axis=1)]).T  # ndim = 2\n",
    "\n",
    "# Create Class: LogisticRegressionClassifier\n",
    "#Logistic Regression Classifier using Softmax.\n",
    "# w0 is the bias unit\n",
    "# w is the weight vector\n",
    "# x is the feature vector of 1 training sample,\n",
    "class LogisticRegressionClassifier(object):\n",
    "    #Initialize the classifier, setting input, labels, W and bias\n",
    "    def __init__(self, input, labels):\n",
    "        self.x = input\n",
    "        self.y = labels\n",
    "        # initialize W 0\n",
    "        # random?\n",
    "        self.W = numpy.zeros((input.shape[1], labels.shape[1])) \n",
    "        # initialize bias 0\n",
    "        self.bias = numpy.zeros(labels.shape[1]) \n",
    "        \n",
    "\n",
    "        \n",
    "    def train(self,L2_reg ,learning_rate=0.1):\n",
    "        prob_y_given_x = softmax(numpy.dot(self.x, self.W) + self.bias)\n",
    "        d_y = self.y - prob_y_given_x\n",
    "        #print(\"d_y is\",d_y)\n",
    "        self.W += learning_rate * numpy.dot(self.x.T, d_y) - learning_rate * L2_reg * self.W\n",
    "        self.bias += learning_rate * numpy.mean(d_y, axis=0)\n",
    "\n",
    "        \n",
    "    def negative_log_likelihood(self):\n",
    "        softmax_activation = softmax(numpy.dot(self.x, self.W) + self.bias)\n",
    "\n",
    "        result = - numpy.mean(\n",
    "            numpy.sum(self.y * numpy.log(softmax_activation) +\n",
    "            (1 - self.y) * numpy.log(1 - softmax_activation), axis=1))\n",
    "        \n",
    "        return result\n",
    "\n",
    "\n",
    "    def predict(self, x):\n",
    "        return softmax(numpy.dot(x, self.W) + self.bias)        \n",
    "# END of LogisticRegressionClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#In order to apply the algorithm to the aforementioned datasets and calculate the the percentage of the misclassified\n",
    "#examples, the following helper functions were implemented:\n",
    "# Apply Logistic Regression to a Dataset\n",
    "def apply_lr(x_train, y_train, x_test, L2_regv, learning_rate=0.01, epochs=200):\n",
    "    # construct LogisticRegression\n",
    "    # creates class object\n",
    "    classifier = LogisticRegressionClassifier(input = x_train, labels = y_train)\n",
    "    # train\n",
    "    for i in range(epochs):\n",
    "#         if (i%10==0):\n",
    "#             print(i)\n",
    "        classifier.train(L2_reg = L2_regv,learning_rate = learning_rate)\n",
    "        cost = classifier.negative_log_likelihood()\n",
    "        # print( '[Training epoch %d] cost: ' % i, cost)\n",
    "        # learning_rate *= 0.99 #decay learning rate after every epoch\n",
    "    # predict\n",
    "    # print( classifier.predict(x_test))\n",
    "    return classifier.predict(x_test)\n",
    "    \n",
    "# Compute the percentage of misclassified examples given the real \n",
    "# and the predicted classes\n",
    "def compute_misclassification_percentage(y,predicted_y):\n",
    "    if y.shape != predicted_y.shape:\n",
    "        return( 'Error! Mismatching dimensions of arrays' )    \n",
    "    y_class = numpy.argmax(y, axis=1)\n",
    "    predicted_y_class = numpy.argmax(predicted_y, axis=1) \n",
    "    percentage = 100 * (y_class != predicted_y_class).sum().astype(float) / y_class.shape[0] \n",
    "    print('Misclassifiction percentage: %f%%' % percentage)\n",
    "    print(\"Confusion matrix:\\n%s\" % confusion_matrix(y_class, predicted_y_class))\n",
    "    print(\"Classification report: \\n%s\" % classification_report(y_class, predicted_y_class))\n",
    "    return percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "true_labels=list(labeled_train_data.index)\n",
    "y_train=[np.repeat(0,10)]*len(true_labels)\n",
    "for i in range(len(y_train)):\n",
    "    k=true_labels[i]\n",
    "    a=list(np.repeat(0,k))\n",
    "    a.append(1)\n",
    "    y_train[i]=np.array(a+list(np.repeat(0,9-k)))\n",
    "y_train=np.array(y_train)\n",
    "################\n",
    "true_labels_2=test_labels\n",
    "y_test=[np.repeat(0,10)]*len(true_labels_2)\n",
    "for i in range(len(y_test)):\n",
    "    k=true_labels_2[i]\n",
    "    a=list(np.repeat(0,k))\n",
    "    a.append(1)\n",
    "    y_test[i]=np.array(a+list(np.repeat(0,9-k)))\n",
    "y_test=np.array(y_test)\n",
    "#################\n",
    "x_train=np.array(train_data)/255.\n",
    "#################\n",
    "x_test=numpy.asarray(test_data)/255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def apply_lr_to_mnist(x_train, y_train, x_test, y_test, L2_reg , learning_rate=0.00001, epochs=200):\n",
    "\n",
    "    predictions = apply_lr(x_train, y_train, x_test,L2_reg,\n",
    "                           learning_rate=learning_rate,\n",
    "                           epochs=epochs)\n",
    "             \n",
    "    return compute_misclassification_percentage(y_test,predictions)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassifiction percentage: 22.040000%\n",
      "Confusion matrix:\n",
      "[[ 940    0    1    3    0    0    8    1   27    0]\n",
      " [   0 1038    1    3    0    0    4    1   88    0]\n",
      " [  48   37  719   25   18    0   20   18  143    4]\n",
      " [  17    5   15  817    1    1    8   15  125    6]\n",
      " [   8   12    4    0  774    0   18    1   57  108]\n",
      " [  96   19    1  166   17  141   19   18  390   25]\n",
      " [  62    7   18    1   11    1  816    0   42    0]\n",
      " [  14   47    7    0   11    0    1  855   55   38]\n",
      " [  13    8    2   27    6    0    4   10  898    6]\n",
      " [  28   12    7    8   48    0    1   35   72  798]]\n",
      "Classification report: \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.77      0.96      0.85       980\n",
      "          1       0.88      0.91      0.89      1135\n",
      "          2       0.93      0.70      0.80      1032\n",
      "          3       0.78      0.81      0.79      1010\n",
      "          4       0.87      0.79      0.83       982\n",
      "          5       0.99      0.16      0.27       892\n",
      "          6       0.91      0.85      0.88       958\n",
      "          7       0.90      0.83      0.86      1028\n",
      "          8       0.47      0.92      0.63       974\n",
      "          9       0.81      0.79      0.80      1009\n",
      "\n",
      "avg / total       0.83      0.78      0.77     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mnist_lr_misc_perc = apply_lr_to_mnist(x_train,y_train,x_test,y_test, L2_reg=0.0,learning_rate=0.00001, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation - Regularization Parameter\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "regul_range_list=np.arange(-10,10,1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "regul_range_list=np.arange(-0.1,0.1,0.01)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(x_train, y_train, test_size=0.2, random_state=42)\n",
    "min=100\n",
    "for i in regul_range_list:\n",
    "    print(\"for regularization parametor\",i)\n",
    "    mnist_lr_misc_perc = apply_lr_to_mnist(X_train,Y_train,X_test,Y_test, L2_reg=i,learning_rate=0.00001, epochs=200)\n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural net "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def activation_f(x):\n",
    "    import numpy as np\n",
    "    output=np.log(1+np.exp(x))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def activation2_f(x):\n",
    "    import numpy as np\n",
    "    output=(np.exp(x)-np.exp(np.negative(x)))/(np.exp(x)+np.exp(np.negative(x)))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax2(w):\n",
    "    w = np.array(w)\n",
    "    maxes = np.amax(w, axis=1)\n",
    "    maxes = maxes.reshape(maxes.shape[0], 1)\n",
    "    e = np.exp(w - maxes)\n",
    "    dist = e / np.sum(e, axis=1).reshape(maxes.shape[0], 1)\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_softmax(x):\n",
    "    import numpy as np\n",
    "    x=softmax2(x)\n",
    "    gradient=x.copy()\n",
    "    for i in range(len(x)):\n",
    "        for j in range(len(x)):\n",
    "            if i == j:\n",
    "                gradient[i] = x[i] * (1-x[i])\n",
    "            else: \n",
    "                gradient[i] = -x[i]*x[j]\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train=labeled_train_data.index\n",
    "X=np.asarray(train_data)\n",
    "y=np.zeros(60000)\n",
    "for i in range(len(y_train)):\n",
    "    y[i]=np.asarray(y_train[i],dtype=int)\n",
    "y=y.reshape(-1,1)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y_onehot=y_train\n",
    "X=np.asarray(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "y_onehot = encoder.fit_transform(y)\n",
    "y_onehot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3, 785), (10, 4))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size = 784\n",
    "hidden_size = 3\n",
    "num_labels = 10\n",
    "regularization_parameter = 1\n",
    "\n",
    "# randomly initialize a parameter array of the size of the full network's parameters\n",
    "params = (np.random.random(size=hidden_size * (input_size + 1) + num_labels * (hidden_size + 1)) - 0.5) * 0.25\n",
    "\n",
    "m = X.shape[0]\n",
    "X = np.matrix(X)\n",
    "y = np.matrix(y)\n",
    "\n",
    "# unravel the parameter array into parameter matrices for each layer\n",
    "theta1 = np.matrix(np.reshape(params[:hidden_size * (input_size + 1)], (hidden_size, (input_size + 1))))\n",
    "theta2 = np.matrix(np.reshape(params[hidden_size * (input_size + 1):], (num_labels, (hidden_size + 1))))\n",
    "\n",
    "theta1.shape, theta2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_propagate(X, theta1, theta2):\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    a1 = np.insert(X, 0, values=np.ones(m), axis=1)\n",
    "    z2 = a1 * theta1.T\n",
    "    a2 = np.insert(activation_f(z2), 0, values=np.ones(m), axis=1)\n",
    "    z3 = a2 * theta2.T\n",
    "    h = softmax2(z3)\n",
    "    \n",
    "    return a1, z2, a2, z3, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_entropy(params, input_size, hidden_size, num_labels, X, y, regularization_parameter):\n",
    "    m = X.shape[0]\n",
    "    n=X.shape[1]\n",
    "    X = np.matrix(X)\n",
    "    y = np.matrix(y)\n",
    "    \n",
    "    # reshape the parameter array into parameter matrices for each layer\n",
    "    theta1 = np.matrix(np.reshape(params[:hidden_size * (input_size + 1)], (hidden_size, (input_size + 1))))\n",
    "    theta2 = np.matrix(np.reshape(params[hidden_size * (input_size + 1):], (num_labels, (hidden_size + 1))))\n",
    "    \n",
    "    # run the feed-forward pass\n",
    "    a1, z2, a2, z3, h = forward_propagate(X, theta1, theta2)\n",
    "    \n",
    "    # compute the cost\n",
    "    J = 0.0\n",
    "    for i in range(m):\n",
    "        for j in range(10):\n",
    "            aux = np.multiply(y[i,j], np.log(h[i,j]))\n",
    "            J += aux\n",
    "    J=J/m\n",
    "    \n",
    "    # add the cost regularization term\n",
    "    J -= (float(regularization_parameter) / 2) * (np.sum(np.power(theta1[:,1:], 2)) + np.sum(np.power(theta2[:,1:], 2)))\n",
    "    \n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 785), (60000, 3), (60000, 4), (60000, 10), (60000, 10))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1, z2, a2, z3, h = forward_propagate(X, theta1, theta2)\n",
    "a1.shape, z2.shape, a2.shape, z3.shape, h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-25.259446391554494"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy(params, input_size, hidden_size, num_labels, X, y_onehot, regularization_parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backprop1(params, input_size, hidden_size, num_labels, X, y, regularization_parameter):\n",
    "    m = X.shape[0]\n",
    "    X = np.matrix(X)\n",
    "    y = np.matrix(y)\n",
    "    \n",
    "    # reshape the parameter array into parameter matrices for each layer\n",
    "    theta1 = np.matrix(np.reshape(params[:hidden_size * (input_size + 1)], (hidden_size, (input_size + 1))))\n",
    "    theta2 = np.matrix(np.reshape(params[hidden_size * (input_size + 1):], (num_labels, (hidden_size + 1))))\n",
    "    \n",
    "    # run the feed-forward pass\n",
    "    a1, z2, a2, z3, h = forward_propagate(X, theta1, theta2)\n",
    "    \n",
    "    # initializations\n",
    "    J = 0\n",
    "    delta1 = np.zeros(theta1.shape)  # (25, 401)\n",
    "    delta2 = np.zeros(theta2.shape)  # (10, 26)\n",
    "    \n",
    "   \n",
    "    J = cross_entropy(params, input_size, hidden_size, num_labels, X, y_onehot, regularization_parameter)\n",
    "    \n",
    "    # perform backpropagation\n",
    "    for t in range(m):\n",
    "        a1t = a1[t,:]  # (1, 401)\n",
    "        z2t = z2[t,:]  # (1, 25)\n",
    "        a2t = a2[t,:]  # (1, 26)\n",
    "        ht = h[t,:]  # (1, 10)\n",
    "        yt = y[t,:]  # (1, 10)\n",
    "        \n",
    "        d3t = ht - yt  # (1, 10)\n",
    "        \n",
    "        z2t = np.insert(z2t, 0, values=np.ones(1))  # (1, 26)\n",
    "        d2t = np.multiply((theta2.T * d3t.T).T, gradient_softmax(z2t))  # (1, 26)\n",
    "        \n",
    "        delta1 = delta1 + (d2t[:,1:]).T * a1t\n",
    "        delta2 = delta2 + d3t.T * a2t\n",
    "        \n",
    "    delta1 = delta1 / m\n",
    "    delta2 = delta2 / m\n",
    "    \n",
    "     # add the gradient regularization term\n",
    "    delta1[:,1:] = delta1[:,1:] + (theta1[:,1:] * regularization_parameter) / m\n",
    "    delta2[:,1:] = delta2[:,1:] + (theta2[:,1:] * regularization_parameter) / m\n",
    "    \n",
    "    # unravel the gradient matrices into a single array\n",
    "    grad = np.concatenate((np.ravel(delta1), np.ravel(delta2)))\n",
    "    \n",
    "    return J, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-25.259446391554494, (2395,))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "J, grad = backprop1(params, input_size, hidden_size, num_labels, X, y_onehot, regularization_parameter)\n",
    "J, grad.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-b3fcc5421674>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[1;31m# minimize the objective function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m fmin = minimize(fun=backprop1, x0=params, args=(input_size, hidden_size, num_labels, X, y_onehot, regularization_parameter), \n\u001b[0;32m----> 5\u001b[0;31m                 method='TNC', jac=True, options={'maxiter': 250})\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mfmin\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\scipy\\optimize\\_minimize.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'tnc'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m         return _minimize_tnc(fun, x0, args, jac, bounds, callback=callback,\n\u001b[0;32m--> 453\u001b[0;31m                              **options)\n\u001b[0m\u001b[1;32m    454\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'cobyla'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_minimize_cobyla\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconstraints\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\scipy\\optimize\\tnc.py\u001b[0m in \u001b[0;36m_minimize_tnc\u001b[0;34m(fun, x0, args, jac, bounds, eps, scale, offset, mesg_num, maxCGit, maxiter, eta, stepmx, accuracy, minfev, ftol, xtol, gtol, rescale, disp, callback, **unknown_options)\u001b[0m\n\u001b[1;32m    407\u001b[0m                                         \u001b[0moffset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxCGit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxfun\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m                                         \u001b[0meta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstepmx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfmin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mftol\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m                                         xtol, pgtol, rescale, callback)\n\u001b[0m\u001b[1;32m    410\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m     \u001b[0mfunv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjacv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\scipy\\optimize\\tnc.py\u001b[0m in \u001b[0;36mfunc_and_grad\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    369\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m             \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjac\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\scipy\\optimize\\optimize.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mfg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjac\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-f52011a86bfa>\u001b[0m in \u001b[0;36mbackprop1\u001b[0;34m(params, input_size, hidden_size, num_labels, X, y, regularization_parameter)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mz2t\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz2t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# (1, 26)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0md2t\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtheta2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0md3t\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz2t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# (1, 26)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mdelta1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdelta1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0md2t\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0ma1t\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\numpy\\matrixlib\\defmatrix.py\u001b[0m in \u001b[0;36m__mul__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m             \u001b[1;31m# This promotes 1-D vectors to row vectors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m             \u001b[1;32mreturn\u001b[0m \u001b[0mN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0masmatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misscalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'__rmul__'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "# minimize the objective function\n",
    "fmin = minimize(fun=backprop1, x0=params, args=(input_size, hidden_size, num_labels, X, y_onehot, regularization_parameter), \n",
    "                method='TNC', jac=True, options={'maxiter': 250})\n",
    "fmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fmin' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-2aceab9a78bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtheta1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfmin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mhidden_size\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minput_size\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minput_size\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtheta2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfmin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mhidden_size\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minput_size\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnum_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhidden_size\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0ma1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mforward_propagate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtheta1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtheta2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fmin' is not defined"
     ]
    }
   ],
   "source": [
    "X = np.matrix(X)\n",
    "theta1 = np.matrix(np.reshape(fmin.x[:hidden_size * (input_size + 1)], (hidden_size, (input_size + 1))))\n",
    "theta2 = np.matrix(np.reshape(fmin.x[hidden_size * (input_size + 1):], (num_labels, (hidden_size + 1))))\n",
    "\n",
    "a1, z2, a2, z3, h = forward_propagate(X, theta1, theta2)\n",
    "y_pred = np.array(np.argmax(h, axis=1) + 1)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "correct = [1 if a == b else 0 for (a, b) in zip(y_pred, y)]\n",
    "accuracy = (sum(map(int, correct)) / float(len(correct)))\n",
    "#print 'accuracy = {0}%'.format(accuracy * 100)\n",
    "accuracy"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
